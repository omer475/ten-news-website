#!/usr/bin/env python3
"""
TEN NEWS - LIVE CONTINUOUS SYSTEM
Runs RSS feeds every 10 minutes and publishes to Supabase
"""

import os
import sys
import time
import signal
import threading
from datetime import datetime
from typing import List, Dict

# Load environment variables from .env file
try:
    from dotenv import load_dotenv
    load_dotenv()
    print("‚úÖ Loaded .env file")
except ImportError:
    print("‚ö†Ô∏è  python-dotenv not available, using system environment variables")

# Import modules after setting environment variables
from step1_gemini_news_scoring_filtering import score_news_articles_step1
from step2_scrapingbee_full_article_fetching import ScrapingBeeArticleFetcher, FetcherConfig
from step3_gemini_component_selection import GeminiComponentSelector, ComponentConfig
from step4_perplexity_dynamic_context_search import PerplexityContextSearcher, PerplexityConfig
from step5_claude_final_writing_formatting import ClaudeFinalWriter, WriterConfig
from supabase_storage import save_articles_to_supabase

class LiveNewsSystem:
    def __init__(self):
        self.running = True
        self.cycle_count = 0
        self.total_articles_published = 0
        
        # Initialize AI components
        self.scrapingbee_fetcher = ScrapingBeeArticleFetcher(
            api_key=os.getenv('SCRAPINGBEE_API_KEY'),
            config=FetcherConfig(parallel_workers=5)
        )
        
        self.component_selector = GeminiComponentSelector(
            api_key=os.getenv('GOOGLE_API_KEY'),
            config=ComponentConfig()
        )
        
        self.context_searcher = PerplexityContextSearcher(
            api_key=os.getenv('PERPLEXITY_API_KEY'),
            config=PerplexityConfig()
        )
        
        self.final_writer = ClaudeFinalWriter(
            api_key=os.getenv('CLAUDE_API_KEY'),
            config=WriterConfig()
        )
        
        print("üöÄ TEN NEWS LIVE SYSTEM INITIALIZED")
        print("=" * 50)
        print("‚úÖ All AI components loaded")
        print("‚úÖ Supabase connection configured")
        print("‚úÖ RSS fetcher ready")
        print("=" * 50)
    
    def fetch_new_rss_articles(self):
        """Fetch new RSS articles"""
        print(f"\nüì∞ [{datetime.now().strftime('%H:%M:%S')}] FETCHING NEW RSS ARTICLES")
        print("-" * 40)
        
        try:
            from rss_fetcher import OptimizedRSSFetcher
            
            fetcher = OptimizedRSSFetcher()
            results = fetcher._parallel_fetch_all_sources()
            
            print(f"‚úÖ RSS Fetch Complete:")
            print(f"   üìä Total articles found: {results['total_articles']}")
            print(f"   üÜï New articles: {results['new_articles']}")
            print(f"   üì∞ Sources fetched: {results['sources_fetched']}")
            print(f"   ‚ùå Failed sources: {results['failed_sources']}")
            
            return results['new_articles']
            
        except Exception as e:
            print(f"‚ùå RSS fetch failed: {e}")
            return 0
    
    def get_new_unread_articles(self, start_time: str, expected_count: int) -> List[Dict]:
        """Get ONLY the new articles from the current RSS fetch cycle"""
        try:
            import sqlite3
            conn = sqlite3.connect('ten_news.db')
            cursor = conn.cursor()
            
            # Get ONLY articles fetched after the cycle start time - limit to expected count
            cursor.execute('''
                SELECT title, source, description, url, fetched_at, id, image_url
                FROM articles 
                WHERE published = 0 
                AND fetched_at > ?
                ORDER BY fetched_at DESC
                LIMIT ?
            ''', (start_time, expected_count))
            
            articles = []
            article_ids = []
            for row in cursor.fetchall():
                articles.append({
                    'title': row[0],
                    'source': row[1], 
                    'text': row[2] or '',
                    'url': row[3],
                    'fetched_at': row[4],
                    'id': row[5],
                    'image_url': row[6] or ''  # Include image_url from database
                })
                article_ids.append(row[5])
            
            # Mark these articles as "processing" to avoid double-processing
            if article_ids:
                placeholders = ','.join(['?' for _ in article_ids])
                cursor.execute(f'''
                    UPDATE articles 
                    SET published = -1 
                    WHERE id IN ({placeholders})
                ''', article_ids)
                conn.commit()
            
            conn.close()
            return articles
            
        except Exception as e:
            print(f"‚ùå Error loading new articles: {e}")
            return []
    
    def process_articles_pipeline(self, articles: List[Dict]) -> List[Dict]:
        """Process articles through the 5-step AI pipeline"""
        
        if not articles:
            return []
        
        print(f"\nü§ñ PROCESSING {len(articles)} ARTICLES THROUGH AI PIPELINE")
        print("-" * 50)
        
        try:
            # Step 1: Gemini Scoring
            print("üîÑ Step 1: Gemini scoring & filtering...")
            batch_results = score_news_articles_step1(articles, os.getenv('GOOGLE_API_KEY'))
            approved_articles = batch_results['approved']
            
            if not approved_articles:
                print("‚ùå No articles approved by Gemini")
                return []
            
            print(f"‚úÖ {len(approved_articles)} articles approved")
            
            # Step 2: ScrapingBee Fetching
            print("üîÑ Step 2: ScrapingBee full article fetching...")
            full_articles = self.scrapingbee_fetcher.fetch_batch_parallel(approved_articles)
            
            if not full_articles:
                print("‚ùå No articles fetched by ScrapingBee")
                return []
            
            print(f"‚úÖ {len(full_articles)} articles fetched")
            
            # Step 3: Component Selection
            print("üîÑ Step 3: Gemini component selection...")
            articles_with_components = self.component_selector.select_components_batch(full_articles)
            
            print(f"‚úÖ {len(articles_with_components)} articles processed")
            
            # Step 4: Context Search
            print("üîÑ Step 4: Perplexity context search...")
            articles_with_context = self.context_searcher.search_all_articles(articles_with_components)
            
            print(f"‚úÖ {len(articles_with_context)} articles processed")
            
            # Step 5: Final Writing
            print("üîÑ Step 5: Claude final writing...")
            final_articles = self.final_writer.write_all_articles(articles_with_context)
            
            print(f"‚úÖ {len(final_articles)} final articles written")
            
            return final_articles
            
        except Exception as e:
            print(f"‚ùå Pipeline processing failed: {e}")
            return []
    
    def publish_to_supabase(self, articles: List[Dict]) -> int:
        """Publish articles to Supabase"""
        if not articles:
            return 0
        
        print(f"\nüåç PUBLISHING {len(articles)} ARTICLES TO SUPABASE")
        print("-" * 40)
        
        try:
            # Convert articles to Supabase format - FULL FIELD MAPPING
            articles_to_publish = []
            article_ids = []
            for article in articles:
                db_article = {
                    # Core fields
                    'url': article.get('url', ''),
                    'guid': article.get('guid', ''),
                    'source': article.get('source', 'Unknown'),
                    'title': article.get('title', ''),
                    'description': article.get('description', ''),
                    'content': article.get('text', ''),
                    'image_url': article.get('image_url', ''),
                    'author': article.get('author', ''),
                    'published_date': article.get('published_date') or article.get('published_time'),
                    
                    # AI scoring
                    'score': float(article.get('score', 0)),
                    'ai_final_score': float(article.get('score', 0)),
                    'ai_category': article.get('category', 'World News'),
                    
                    # Publishing
                    'category': article.get('category', 'World News'),
                    'emoji': article.get('emoji', 'üì∞'),
                    
                    # Enhanced content
                    'article': article.get('detailed_text', ''),  # NEW: Detailed article text (max 200 words)
                    'summary_bullets': article.get('summary_bullets', []),  # NEW: Bullet points
                    'timeline': article.get('timeline', []),
                    'details': article.get('details', []),
                    'graph': article.get('graph', {}),
                    'map': article.get('map', {}),
                    
                    # Timestamps
                    'publishedAt': datetime.now().isoformat(),
                    'image_extraction_method': article.get('image_extraction_method', '')
                }
                articles_to_publish.append(db_article)
                if 'id' in article:
                    article_ids.append(article['id'])
            
            # Publish to Supabase
            success = save_articles_to_supabase(articles_to_publish, source_part=1)
            success_count = len(articles_to_publish) if success else 0
            
            # Mark articles as published in local database
            if success and article_ids:
                import sqlite3
                conn = sqlite3.connect('ten_news.db')
                cursor = conn.cursor()
                placeholders = ','.join(['?' for _ in article_ids])
                cursor.execute(f'''
                    UPDATE articles 
                    SET published = 1 
                    WHERE id IN ({placeholders})
                ''', article_ids)
                conn.commit()
                conn.close()
            
            print(f"‚úÖ Publishing Complete:")
            print(f"   üì§ Articles published: {success_count}")
            print(f"   üåç Live on: https://tennews.ai")
            
            return success_count
            
        except Exception as e:
            print(f"‚ùå Publishing failed: {e}")
            return 0
    
    def run_cycle(self):
        """Run one complete cycle: RSS ‚Üí Process ‚Üí Publish"""
        self.cycle_count += 1
        
        cycle_start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        print(f"\n{'='*60}")
        print(f"üîÑ CYCLE #{self.cycle_count} - {cycle_start_time}")
        print(f"{'='*60}")
        
        try:
            # Step 1: Fetch new RSS articles
            new_articles_count = self.fetch_new_rss_articles()
            
            if new_articles_count == 0:
                print("‚è≠Ô∏è No new articles - skipping cycle")
                return
            
            # Step 2: Get ONLY the new articles from current RSS fetch
            new_articles = self.get_new_unread_articles(cycle_start_time, new_articles_count)
            
            if not new_articles:
                print("‚è≠Ô∏è No new articles from current RSS fetch - skipping cycle")
                return
            
            print(f"üìä Processing {len(new_articles)} NEW articles from current RSS fetch (expected: {new_articles_count})")
            
            # Step 3: Process through AI pipeline
            final_articles = self.process_articles_pipeline(new_articles)
            
            if not final_articles:
                print("‚è≠Ô∏è No articles processed - skipping cycle")
                return
            
            # Step 4: Publish to Supabase
            published_count = self.publish_to_supabase(final_articles)
            
            # Update totals
            self.total_articles_published += published_count
            
            # Cycle summary
            print(f"\n‚úÖ CYCLE #{self.cycle_count} COMPLETE")
            print(f"   üì∞ RSS fetch: {new_articles_count} new articles")
            print(f"   üìä Processed: {len(new_articles)} articles")
            print(f"   ‚úçÔ∏è Finalized: {len(final_articles)} articles")
            print(f"   üåç Published: {published_count} articles")
            print(f"   üìà Total published: {self.total_articles_published} articles")
            
        except Exception as e:
            print(f"‚ùå Cycle #{self.cycle_count} failed: {e}")
    
    def run_continuous(self):
        """Run the system continuously every 10 minutes"""
        print(f"\nüöÄ STARTING CONTINUOUS LIVE SYSTEM")
        print(f"‚è∞ Running every 10 minutes")
        print(f"üõë Press Ctrl+C to stop")
        print(f"{'='*60}")
        
        while self.running:
            try:
                self.run_cycle()
                
                if self.running:
                    print(f"\n‚è∞ Waiting 10 minutes until next cycle...")
                    next_cycle_time = datetime.fromtimestamp(datetime.now().timestamp() + 600)
                    print(f"üïê Next cycle at: {next_cycle_time.strftime('%H:%M:%S')}")
                    
                    # Wait 10 minutes (600 seconds)
                    for i in range(600):
                        if not self.running:
                            break
                        time.sleep(1)
                
            except KeyboardInterrupt:
                print(f"\nüõë Stopping live system...")
                self.running = False
                break
            except Exception as e:
                print(f"‚ùå System error: {e}")
                print(f"‚è∞ Retrying in 5 minutes...")
                time.sleep(300)  # Wait 5 minutes before retry
        
        print(f"\n‚úÖ LIVE SYSTEM STOPPED")
        print(f"üìä Total cycles completed: {self.cycle_count}")
        print(f"üìà Total articles published: {self.total_articles_published}")

def signal_handler(signum, frame):
    """Handle Ctrl+C gracefully"""
    print(f"\nüõë Received interrupt signal...")
    sys.exit(0)

if __name__ == "__main__":
    # Set up signal handler for graceful shutdown
    signal.signal(signal.SIGINT, signal_handler)
    
    # Create and run live system
    live_system = LiveNewsSystem()
    live_system.run_continuous()
